{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a998ee72c40e45e686b173a9f4b75b09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_203cc35fc7e34526b105d6dcaf058a31","IPY_MODEL_7c54c3d66e4d4472b75b2671962e46e2","IPY_MODEL_e1666970f2f645c69132d5be653a2dae"],"layout":"IPY_MODEL_7b1f33790e864c149b1f84fd22e97e93"}},"203cc35fc7e34526b105d6dcaf058a31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0896d56b24de4ccdaa8e5e3d560d7b59","placeholder":"​","style":"IPY_MODEL_20bf791327dd456e88e0240caddc4aa9","value":"model.safetensors: 100%"}},"7c54c3d66e4d4472b75b2671962e46e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_97c15063267245daa28674ab35e24806","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f365ce7a311249968c38da5600e2ac71","value":440449768}},"e1666970f2f645c69132d5be653a2dae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ef730398cf54263a2dd8369c14ca00c","placeholder":"​","style":"IPY_MODEL_36014c2b02f44695b007ca4b66646ee2","value":" 440M/440M [00:06&lt;00:00, 95.9MB/s]"}},"7b1f33790e864c149b1f84fd22e97e93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0896d56b24de4ccdaa8e5e3d560d7b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20bf791327dd456e88e0240caddc4aa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97c15063267245daa28674ab35e24806":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f365ce7a311249968c38da5600e2ac71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7ef730398cf54263a2dd8369c14ca00c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36014c2b02f44695b007ca4b66646ee2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10605270,"sourceType":"datasetVersion","datasetId":6564896}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"id":"aUucJ1iMG8d7","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:03.117406Z","iopub.execute_input":"2025-01-30T02:19:03.117756Z","iopub.status.idle":"2025-01-30T02:19:11.245990Z","shell.execute_reply.started":"2025-01-30T02:19:03.117728Z","shell.execute_reply":"2025-01-30T02:19:11.244904Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVGVmIjtV6je","outputId":"fa62ca49-c6a5-41a3-a3af-1e56b417dcab","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:11.247444Z","iopub.execute_input":"2025-01-30T02:19:11.247955Z","iopub.status.idle":"2025-01-30T02:19:11.330723Z","shell.execute_reply.started":"2025-01-30T02:19:11.247922Z","shell.execute_reply":"2025-01-30T02:19:11.329817Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/scdataset/Vunrebility_Dataset.csv\")","metadata":{"id":"AbYK1GJ5HAo2","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:11.332061Z","iopub.execute_input":"2025-01-30T02:19:11.332341Z","iopub.status.idle":"2025-01-30T02:19:12.238950Z","shell.execute_reply.started":"2025-01-30T02:19:11.332317Z","shell.execute_reply":"2025-01-30T02:19:12.238047Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"4qZXo0n9Hyxc","outputId":"ce91b6a7-8957-44a9-82b5-9e9d2f0c1258","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.240425Z","iopub.execute_input":"2025-01-30T02:19:12.240687Z","iopub.status.idle":"2025-01-30T02:19:12.265529Z","shell.execute_reply.started":"2025-01-30T02:19:12.240664Z","shell.execute_reply":"2025-01-30T02:19:12.264528Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0   filename                                               code  \\\n0           0  33790.sol  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...   \n1           1  31454.sol  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...   \n2           2  40744.sol  contract SendBalance {\\n mapping (address => u...   \n3           3  39290.sol  /**\\n * Originally from https://github.com/Con...   \n4           4  39358.sol  pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...   \n\n                        label  label_encoded  \n0  ./Dataset/reentrancy (RE)/              5  \n1  ./Dataset/reentrancy (RE)/              5  \n2  ./Dataset/reentrancy (RE)/              5  \n3  ./Dataset/reentrancy (RE)/              5  \n4  ./Dataset/reentrancy (RE)/              5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>code</th>\n      <th>label</th>\n      <th>label_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>33790.sol</td>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>./Dataset/reentrancy (RE)/</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>31454.sol</td>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>./Dataset/reentrancy (RE)/</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>40744.sol</td>\n      <td>contract SendBalance {\\n mapping (address =&gt; u...</td>\n      <td>./Dataset/reentrancy (RE)/</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>39290.sol</td>\n      <td>/**\\n * Originally from https://github.com/Con...</td>\n      <td>./Dataset/reentrancy (RE)/</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>39358.sol</td>\n      <td>pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...</td>\n      <td>./Dataset/reentrancy (RE)/</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df = df.drop(columns = ['Unnamed: 0', 'filename', 'label'])","metadata":{"id":"O2g7Ee8JIEuK","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.266470Z","iopub.execute_input":"2025-01-30T02:19:12.266787Z","iopub.status.idle":"2025-01-30T02:19:12.278708Z","shell.execute_reply.started":"2025-01-30T02:19:12.266755Z","shell.execute_reply":"2025-01-30T02:19:12.277725Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"TClxgdchRyeJ","outputId":"1cd6b29a-a6aa-4aac-a78f-08ba103dc4b7","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.279788Z","iopub.execute_input":"2025-01-30T02:19:12.280116Z","iopub.status.idle":"2025-01-30T02:19:12.296592Z","shell.execute_reply.started":"2025-01-30T02:19:12.280064Z","shell.execute_reply":"2025-01-30T02:19:12.295675Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                code  label_encoded\n0  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...              5\n1  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...              5\n2  contract SendBalance {\\n mapping (address => u...              5\n3  /**\\n * Originally from https://github.com/Con...              5\n4  pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...              5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>label_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>contract SendBalance {\\n mapping (address =&gt; u...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/**\\n * Originally from https://github.com/Con...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df['label_encoded'].value_counts()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"id":"ddKtOqRjH0ES","outputId":"8513b7f1-0545-4076-c821-9c28d5070731","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.297608Z","iopub.execute_input":"2025-01-30T02:19:12.297906Z","iopub.status.idle":"2025-01-30T02:19:12.322358Z","shell.execute_reply.started":"2025-01-30T02:19:12.297884Z","shell.execute_reply":"2025-01-30T02:19:12.321432Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label_encoded\n5    1218\n7    1199\n4     590\n0     406\n3     366\n6     312\n1      97\n2      97\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"max_len = 0\nfor i in df['code']:\n    if len(i) > max_len:\n        max_len = len(i)\nprint(max_len)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15eXQKUtIXAS","outputId":"d91d04ce-a855-4ac2-e4f6-e88a92a278e6","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.325437Z","iopub.execute_input":"2025-01-30T02:19:12.325664Z","iopub.status.idle":"2025-01-30T02:19:12.336906Z","shell.execute_reply.started":"2025-01-30T02:19:12.325644Z","shell.execute_reply":"2025-01-30T02:19:12.335928Z"}},"outputs":[{"name":"stdout","text":"108950\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"print(df['code'][0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65Lr4lbRT_c4","outputId":"237822b8-8a00-4fe8-fbf7-ccb649fde724","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.338569Z","iopub.execute_input":"2025-01-30T02:19:12.338815Z","iopub.status.idle":"2025-01-30T02:19:12.351462Z","shell.execute_reply.started":"2025-01-30T02:19:12.338785Z","shell.execute_reply":"2025-01-30T02:19:12.350605Z"}},"outputs":[{"name":"stdout","text":"pragma solidity ^0.4.4;\n\ncontract Token {\n\n    /// @return total amount of tokens\n    function totalSupply(uint256) constant returns (uint256 supply) {}\n\n    /// @param _owner The address from which the balance will be retrieved\n    /// @return The balance\n    function balanceOf(address _owner) constant returns (uint256 balance) {}\n\n    /// @notice send `_value` token to `_to` from `msg.sender`\n    /// @param _to The address of the recipient\n    /// @param _value The amount of token to be transferred\n    /// @return Whether the transfer was successful or not\n    function transfer(address _to, uint256 _value) returns (bool success) {}\n\n    /// @notice send `_value` token to `_to` from `_from` on the condition it is approved by `_from`\n    /// @param _from The address of the sender\n    /// @param _to The address of the recipient\n    /// @param _value The amount of token to be transferred\n    /// @return Whether the transfer was successful or not\n    function transferFrom(address _from, address _to, uint256 _value) returns (bool success) {}\n\n    /// @notice `msg.sender` approves `_addr` to spend `_value` tokens\n    /// @param _spender The address of the account able to transfer the tokens\n    /// @param _value The amount of wei to be approved for transfer\n    /// @return Whether the approval was successful or not\n    function approve(address _spender, uint256 _value) returns (bool success) {}\n\n    /// @param _owner The address of the account owning tokens\n    /// @param _spender The address of the account able to transfer the tokens\n    /// @return Amount of remaining tokens allowed to spent\n    function allowance(address _owner, address _spender) constant returns (uint256 remaining) {}\n\n    event Transfer(address indexed _from, address indexed _to, uint256 _value);\n    event Approval(address indexed _owner, address indexed _spender, uint256 _value);\n    \n}\n\n\n\ncontract StandardToken is Token {\n\n    function transfer(address _to, uint256 _value) returns (bool success) {\n        //Default assumes totalSupply can't be over max (2^256 - 1).\n        //If your token leaves out totalSupply and can issue more tokens as time goes on, you need to check if it doesn't wrap.\n        //Replace the if with this one instead.\n        //if (balances[msg.sender] >= _value && balances[_to] + _value > balances[_to]) {\n        if (balances[msg.sender] >= _value && _value > 0) {\n            balances[msg.sender] -= _value;\n            balances[_to] += _value;\n            Transfer(msg.sender, _to, _value);\n            return true;\n        } else { return false; }\n    }\n\n    function transferFrom(address _from, address _to, uint256 _value) returns (bool success) {\n        //same as above. Replace this line with the following if you want to protect against wrapping uints.\n        //if (balances[_from] >= _value && allowed[_from][msg.sender] >= _value && balances[_to] + _value > balances[_to]) {\n        if (balances[_from] >= _value && allowed[_from][msg.sender] >= _value && _value > 0) {\n            balances[_to] += _value;\n            balances[_from] -= _value;\n            allowed[_from][msg.sender] -= _value;\n            Transfer(_from, _to, _value);\n            return true;\n        } else { return false; }\n    }\n\n    function balanceOf(address _owner) constant returns (uint256 balance) {\n        return balances[_owner];\n    }\n\n    function approve(address _spender, uint256 _value) returns (bool success) {\n        allowed[msg.sender][_spender] = _value;\n        Approval(msg.sender, _spender, _value);\n        return true;\n    }\n\n    function allowance(address _owner, address _spender) constant returns (uint256 remaining) {\n      return allowed[_owner][_spender];\n    }\n\n    mapping (address => uint256) balances;\n    mapping (address => mapping (address => uint256)) allowed;\n    uint256 public totalSupply;\n}\n\n\n//name this contract whatever you'd like\ncontract SALES is StandardToken {\n\n    function () {\n        //if ether is sent to this address, send it back.\n        throw;\n    }\n\n    /* Public variables of the token */\n\n    /*\n    NOTE:\n    The following variables are OPTIONAL vanities. One does not have to include them.\n    They allow one to customise the token contract & in no way influences the core functionality.\n    Some wallets/interfaces might not even bother to look at this information.\n    */\n    string public name;                   //fancy name: eg Simon Bucks\n    uint8 public decimals;                //How many decimals to show. ie. There could 1000 base units with 3 decimals. Meaning 0.980 SBX = 980 base units. It's like comparing 1 wei to 1 ether.\n    string public symbol;                 //An identifier: eg SBX\n    string public version = 'H1.0';       //human 0.1 standard. Just an arbitrary versioning scheme.\n\n//\n// CHANGE THESE VALUES FOR YOUR TOKEN\n//\n\n//make sure this function name matches the contract name above. So if you're token is called TutorialToken, make sure the //contract name above is also TutorialToken instead of ERC20Token\n\n    function SALES(\n        ) {\n        balances[msg.sender] = 1010101010;               // Give the creator all initial tokens (100000 for example)\n        totalSupply =1010101010;                        // Update total supply (100000 for example)\n        name = \"SALES\";                                   // Set the name for display purposes\n        decimals = 2;                            // Amount of decimals for display purposes\n        symbol = \"SALES\";                               // Set the symbol for display purposes\n    }\n\n    /* Approves and then calls the receiving contract */\n    function approveAndCall(address _spender, uint256 _value, bytes _extraData) returns (bool success) {\n        allowed[msg.sender][_spender] = _value;\n        Approval(msg.sender, _spender, _value);\n\n       //call the receiveApproval function on the contract you want to be notified. This crafts the function signature manually so one doesn't have to include a contract in here just for this.\n        //receiveApproval(address _from, uint256 _value, address _tokenContract, bytes _extraData)\n        //it is assumed that when does this that the call *should* succeed, otherwise one would use vanilla approve instead.\n        if(!_spender.call(bytes4(bytes32(sha3(\"receiveApproval(address,uint256,address,bytes)\"))), msg.sender, _value, this, _extraData)) { throw; }\n        return true;\n    }\n}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"#Preprocessing","metadata":{"id":"kCZQIYdkSqv6"}},{"cell_type":"code","source":"df_encoded = pd.get_dummies(df, columns = ['label_encoded'], dtype=int)\ndf_encoded.head()","metadata":{"id":"URgvhxpMq7fC","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"47944c0a-cdc3-4702-aeaa-db82f843bcd2","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.352529Z","iopub.execute_input":"2025-01-30T02:19:12.352787Z","iopub.status.idle":"2025-01-30T02:19:12.383199Z","shell.execute_reply.started":"2025-01-30T02:19:12.352755Z","shell.execute_reply":"2025-01-30T02:19:12.382363Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                code  label_encoded_0  \\\n0  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...                0   \n1  pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...                0   \n2  contract SendBalance {\\n mapping (address => u...                0   \n3  /**\\n * Originally from https://github.com/Con...                0   \n4  pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...                0   \n\n   label_encoded_1  label_encoded_2  label_encoded_3  label_encoded_4  \\\n0                0                0                0                0   \n1                0                0                0                0   \n2                0                0                0                0   \n3                0                0                0                0   \n4                0                0                0                0   \n\n   label_encoded_5  label_encoded_6  label_encoded_7  \n0                1                0                0  \n1                1                0                0  \n2                1                0                0  \n3                1                0                0  \n4                1                0                0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>label_encoded_0</th>\n      <th>label_encoded_1</th>\n      <th>label_encoded_2</th>\n      <th>label_encoded_3</th>\n      <th>label_encoded_4</th>\n      <th>label_encoded_5</th>\n      <th>label_encoded_6</th>\n      <th>label_encoded_7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pragma solidity ^0.4.4;\\n\\ncontract Token {\\n\\...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>contract SendBalance {\\n mapping (address =&gt; u...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/**\\n * Originally from https://github.com/Con...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>pragma solidity ^0.4.4;\\n\\nlibrary ArrayLib{\\n...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"solidity_stopwords = [\n    \"pragma\", \"interface\", \"contract\", \"function\", \"event\", \"modifier\", \"library\", \"using\",\n    \"string\", \"uint8\", \"uint256\", \"address\", \"mapping\", \"bool\", \"require\", \"return\", \"memory\",\n    \"storage\", \"public\", \"internal\", \"view\", \"returns\", \"constant\", \"constructor\",\n    \"_owner\", \"_balances\", \"_allowances\", \"_founder\", \"_marketing\", \"_who\", \"_burntAmount\",\n    \"_from\", \"_to\", \"_value\", \"_timestamp\", \"_bool\", \"msg.sender\", \"totalSupply\",\n    \"balanceOf\", \"transfer\", \"allowance\", \"approve\", \"transferFrom\", \"add\", \"sub\", \"mul\", \"div\",\n    \"mod\", \"changeFounder\", \"setMinter\", \"setFurnace\", \"freezeAccount\",\"solidity\",\"bytes32\"\n]\n\ndef clean_solidity_code(solidity_code):\n    # Remove comments (both single-line and multi-line)\n    cleaned_code = re.sub(r'//.*?$', '', solidity_code, flags=re.MULTILINE)\n    cleaned_code = re.sub(r'/\\*.*?\\*/', '', cleaned_code, flags=re.DOTALL)\n\n    # Remove special characters and punctuation\n    cleaned_code = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_code)\n\n    # Remove extra whitespace and blank lines, and convert to lowercase\n    cleaned_code = '\\n'.join(line.strip().lower() for line in cleaned_code.splitlines() if line.strip())\n\n    # Remove common English stop words\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in cleaned_code.split() if word not in stop_words]\n    tokens = [token for token in tokens if token not in solidity_stopwords]\n    cleaned_code = ' '.join(tokens)\n\n    return cleaned_code","metadata":{"id":"2mJelCQkT7DQ","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.384247Z","iopub.execute_input":"2025-01-30T02:19:12.384568Z","iopub.status.idle":"2025-01-30T02:19:12.391811Z","shell.execute_reply.started":"2025-01-30T02:19:12.384545Z","shell.execute_reply":"2025-01-30T02:19:12.390736Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df['code'] = df['code'].apply(clean_solidity_code)","metadata":{"id":"2BUPPNamDp_z","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:12.392760Z","iopub.execute_input":"2025-01-30T02:19:12.393021Z","iopub.status.idle":"2025-01-30T02:19:17.064323Z","shell.execute_reply.started":"2025-01-30T02:19:12.393000Z","shell.execute_reply":"2025-01-30T02:19:17.063319Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"max_len = 0\nfor i in df['code']:\n    if len(i) > max_len:\n        max_len = len(i)\nprint(max_len)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9WDYGYAElcL","outputId":"f726ca17-2b58-4ff9-cc80-b2e5c8c944de","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:17.065248Z","iopub.execute_input":"2025-01-30T02:19:17.065572Z","iopub.status.idle":"2025-01-30T02:19:17.072334Z","shell.execute_reply.started":"2025-01-30T02:19:17.065540Z","shell.execute_reply":"2025-01-30T02:19:17.071446Z"}},"outputs":[{"name":"stdout","text":"46854\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"df_0 = df_encoded.drop(columns = ['label_encoded_1', 'label_encoded_2', 'label_encoded_3', 'label_encoded_4', 'label_encoded_5', 'label_encoded_6', 'label_encoded_7'])\ndf_1 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_2', 'label_encoded_3', 'label_encoded_4', 'label_encoded_5', 'label_encoded_6', 'label_encoded_7'])\ndf_2 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_3', 'label_encoded_4', 'label_encoded_5', 'label_encoded_6', 'label_encoded_7'])\ndf_3 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_2', 'label_encoded_4', 'label_encoded_5', 'label_encoded_6', 'label_encoded_7'])\ndf_4 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_2', 'label_encoded_3', 'label_encoded_5', 'label_encoded_6', 'label_encoded_7'])\ndf_5 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_2', 'label_encoded_3', 'label_encoded_4', 'label_encoded_6', 'label_encoded_7'])\ndf_6 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_2', 'label_encoded_3', 'label_encoded_4', 'label_encoded_5', 'label_encoded_7'])\ndf_7 = df_encoded.drop(columns = ['label_encoded_0', 'label_encoded_1', 'label_encoded_2', 'label_encoded_3', 'label_encoded_4', 'label_encoded_5', 'label_encoded_6'])","metadata":{"id":"vAmJUeOGD2z4","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:17.073501Z","iopub.execute_input":"2025-01-30T02:19:17.073742Z","iopub.status.idle":"2025-01-30T02:19:17.179290Z","shell.execute_reply.started":"2025-01-30T02:19:17.073721Z","shell.execute_reply":"2025-01-30T02:19:17.178537Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"#Model Building","metadata":{"id":"9qNOA14sDw_c"}},{"cell_type":"code","source":"models =[]","metadata":{"id":"L1lCfyWcXdk8","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:17.180248Z","iopub.execute_input":"2025-01-30T02:19:17.180585Z","iopub.status.idle":"2025-01-30T02:19:17.189346Z","shell.execute_reply.started":"2025-01-30T02:19:17.180546Z","shell.execute_reply":"2025-01-30T02:19:17.188608Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"","metadata":{"id":"xWl3-7bSaUtw","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:17.190253Z","iopub.execute_input":"2025-01-30T02:19:17.190495Z","iopub.status.idle":"2025-01-30T02:19:17.203980Z","shell.execute_reply.started":"2025-01-30T02:19:17.190462Z","shell.execute_reply":"2025-01-30T02:19:17.202942Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# model_temp = AutoModelForSequenceClassification.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"vARch0jSCJMI","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:19:17.204912Z","iopub.execute_input":"2025-01-30T02:19:17.205178Z","iopub.status.idle":"2025-01-30T02:20:16.063239Z","shell.execute_reply.started":"2025-01-30T02:19:17.205158Z","shell.execute_reply":"2025-01-30T02:20:16.062540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e4a5693108b495280810fb405d51bfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f10407c1fb81451ab0409905a21eb743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c2bddad379400e80341532f8eeb7d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601092b60cd34c67bd271a5b06bf5c74"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# params = sum(p.numel() for p in model_temp.parameters() if p.requires_grad)\n# print(f\"The model has {params} trainable parameters.\")\ntokenizer.model_max_length","metadata":{"id":"jopAY2Fo9i5x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"12629a21-40ef-4da6-8907-df576c5f1be9","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.064111Z","iopub.execute_input":"2025-01-30T02:20:16.064456Z","iopub.status.idle":"2025-01-30T02:20:16.069566Z","shell.execute_reply.started":"2025-01-30T02:20:16.064427Z","shell.execute_reply":"2025-01-30T02:20:16.068863Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nbatch_size = 32\nlr = 2e-5\nnum_epochs = 10\n# model_temp.to(device)","metadata":{"id":"lhlxW3GpDZKX","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.070390Z","iopub.execute_input":"2025-01-30T02:20:16.070667Z","iopub.status.idle":"2025-01-30T02:20:16.168652Z","shell.execute_reply.started":"2025-01-30T02:20:16.070638Z","shell.execute_reply":"2025-01-30T02:20:16.167838Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def get_dataloader(text, labels):\n    input_ids = []\n    attention_masks = []\n    \n    for desc in text:\n        encoded_dict = tokenizer.encode_plus(desc, add_special_tokens=True, padding='max_length', truncation=True,\n                                                return_attention_mask=True, return_tensors='pt')\n        input_ids.append(encoded_dict['input_ids'].to(device))\n        attention_masks.append(encoded_dict['attention_mask'].to(device))\n    \n    input_ids = torch.cat(input_ids, dim=0).to(device)\n    attention_mask = torch.cat(attention_masks, dim=0).to(device)\n    labels = torch.tensor(labels.to_numpy()).to(device)\n    \n    dataset = TensorDataset(input_ids, attention_mask, labels)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    return dataloader","metadata":{"id":"cfojm8bmDJ8-","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.169526Z","iopub.execute_input":"2025-01-30T02:20:16.169747Z","iopub.status.idle":"2025-01-30T02:20:16.185761Z","shell.execute_reply.started":"2025-01-30T02:20:16.169728Z","shell.execute_reply":"2025-01-30T02:20:16.185130Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def fine_tune_model(model, train_loader, test_loader):\n    optim = torch.optim.Adam(model.parameters(), lr = lr)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    print(\"Training Model!!!\")\n\n    for epoch in range(num_epochs):\n        correct_predictions = 0\n        total_predictions = 0\n        running_loss = 0.0\n        count = 0\n\n        for input_ids, attention_mask, labels in train_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            optim.zero_grad()\n\n            output = model(input_ids, attention_mask)\n            logits = output.logits\n            loss = criterion(logits, labels)\n            loss.backward()\n            optim.step()\n\n            running_loss += loss.item()\n            count += 1\n\n            _, predicted = torch.max(logits, 1)\n            total_predictions += labels.size(0)\n            correct_predictions += (predicted == labels).sum().item()\n\n        total_loss = running_loss / count\n        accuracy = correct_predictions / total_predictions * 100\n        print(f'Epoch: {epoch + 1}, Training Loss: {running_loss:.4f}, Training Accuracy: {accuracy:.2f}%')\n\n    print(\"Testing Model!!!\")\n\n    with torch.no_grad():\n        correct_predictions = 0\n        total_predictions = 0\n        running_loss = 0.0\n        count = 0\n\n        for input_ids, attention_mask, labels in test_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            output = model(input_ids, attention_mask)\n            logits = output.logits\n            loss = criterion(logits, labels)\n\n            running_loss += loss.item()\n            count += 1\n\n            _, predicted = torch.max(logits, 1)\n            total_predictions += labels.size(0)\n            correct_predictions += (predicted == labels).sum().item()\n\n        accuracy = correct_predictions / total_predictions * 100\n        print(f'Testing Loss: {running_loss:.4f}, Testing Accuracy: {accuracy:.2f}%')","metadata":{"id":"Uv7nrfteFYGG","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.186755Z","iopub.execute_input":"2025-01-30T02:20:16.187094Z","iopub.status.idle":"2025-01-30T02:20:16.203157Z","shell.execute_reply.started":"2025-01-30T02:20:16.187042Z","shell.execute_reply":"2025-01-30T02:20:16.202418Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_model(curr_df, i):\n    x = curr_df['code']\n    y = curr_df[f'label_encoded_{i}']\n    \n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42, stratify=y)\n    \n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    if torch.cuda.device_count() > 1:\n        print(\"No. of GPUs: \", torch.cuda.device_count())\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n    \n    train_loader = get_dataloader(x_train, y_train)\n    test_loader = get_dataloader(x_test, y_test)\n    \n    fine_tune_model(model, train_loader, test_loader)\n    \n    models.append(model)","metadata":{"id":"-MMFuPLQYejE","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.204051Z","iopub.execute_input":"2025-01-30T02:20:16.204342Z","iopub.status.idle":"2025-01-30T02:20:16.220696Z","shell.execute_reply.started":"2025-01-30T02:20:16.204321Z","shell.execute_reply":"2025-01-30T02:20:16.219877Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"datasets = [df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7]\nfor i, dataset in enumerate(datasets):\n    print(f'For dataset {i}')\n    train_model(dataset, i)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309,"referenced_widgets":["a998ee72c40e45e686b173a9f4b75b09","203cc35fc7e34526b105d6dcaf058a31","7c54c3d66e4d4472b75b2671962e46e2","e1666970f2f645c69132d5be653a2dae","7b1f33790e864c149b1f84fd22e97e93","0896d56b24de4ccdaa8e5e3d560d7b59","20bf791327dd456e88e0240caddc4aa9","97c15063267245daa28674ab35e24806","f365ce7a311249968c38da5600e2ac71","7ef730398cf54263a2dd8369c14ca00c","36014c2b02f44695b007ca4b66646ee2"]},"id":"zOGat8YpIj4-","outputId":"5cb16b2c-f2a9-4c33-bc79-6ecc6dcfcffd","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T02:20:16.223354Z","iopub.execute_input":"2025-01-30T02:20:16.223583Z","iopub.status.idle":"2025-01-30T06:43:57.061281Z","shell.execute_reply.started":"2025-01-30T02:20:16.223563Z","shell.execute_reply":"2025-01-30T06:43:57.060325Z"}},"outputs":[{"name":"stdout","text":"For dataset 0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf331230e0b4350aa2546fd31c56e03"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 22.8258, Training Accuracy: 92.50%\nEpoch: 2, Training Loss: 14.5483, Training Accuracy: 95.22%\nEpoch: 3, Training Loss: 13.5086, Training Accuracy: 95.13%\nEpoch: 4, Training Loss: 9.7772, Training Accuracy: 96.85%\nEpoch: 5, Training Loss: 8.0920, Training Accuracy: 97.75%\nEpoch: 6, Training Loss: 5.8565, Training Accuracy: 98.02%\nEpoch: 7, Training Loss: 4.9469, Training Accuracy: 98.60%\nEpoch: 8, Training Loss: 3.8342, Training Accuracy: 98.83%\nEpoch: 9, Training Loss: 5.8883, Training Accuracy: 98.25%\nEpoch: 10, Training Loss: 6.3784, Training Accuracy: 97.78%\nTesting Model!!!\nTesting Loss: 3.6842, Testing Accuracy: 96.50%\nFor dataset 1\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 12.3861, Training Accuracy: 97.72%\nEpoch: 2, Training Loss: 6.9423, Training Accuracy: 97.72%\nEpoch: 3, Training Loss: 6.0652, Training Accuracy: 97.72%\nEpoch: 4, Training Loss: 5.7478, Training Accuracy: 97.72%\nEpoch: 5, Training Loss: 5.0249, Training Accuracy: 97.61%\nEpoch: 6, Training Loss: 4.4214, Training Accuracy: 97.70%\nEpoch: 7, Training Loss: 3.9746, Training Accuracy: 97.58%\nEpoch: 8, Training Loss: 3.8672, Training Accuracy: 97.49%\nEpoch: 9, Training Loss: 3.8382, Training Accuracy: 97.67%\nEpoch: 10, Training Loss: 3.4454, Training Accuracy: 97.87%\nTesting Model!!!\nTesting Loss: 2.7059, Testing Accuracy: 96.62%\nFor dataset 2\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 12.1633, Training Accuracy: 96.82%\nEpoch: 2, Training Loss: 7.4092, Training Accuracy: 97.52%\nEpoch: 3, Training Loss: 6.0898, Training Accuracy: 97.64%\nEpoch: 4, Training Loss: 5.5611, Training Accuracy: 97.90%\nEpoch: 5, Training Loss: 4.7123, Training Accuracy: 97.49%\nEpoch: 6, Training Loss: 4.5344, Training Accuracy: 97.67%\nEpoch: 7, Training Loss: 3.9982, Training Accuracy: 97.52%\nEpoch: 8, Training Loss: 3.8821, Training Accuracy: 97.61%\nEpoch: 9, Training Loss: 3.6134, Training Accuracy: 97.75%\nEpoch: 10, Training Loss: 4.4655, Training Accuracy: 97.61%\nTesting Model!!!\nTesting Loss: 1.7742, Testing Accuracy: 97.32%\nFor dataset 3\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 25.4956, Training Accuracy: 91.45%\nEpoch: 2, Training Loss: 13.4657, Training Accuracy: 95.13%\nEpoch: 3, Training Loss: 7.9954, Training Accuracy: 97.58%\nEpoch: 4, Training Loss: 5.0724, Training Accuracy: 98.42%\nEpoch: 5, Training Loss: 4.4425, Training Accuracy: 98.86%\nEpoch: 6, Training Loss: 3.1287, Training Accuracy: 99.30%\nEpoch: 7, Training Loss: 2.2617, Training Accuracy: 99.39%\nEpoch: 8, Training Loss: 1.7953, Training Accuracy: 99.53%\nEpoch: 9, Training Loss: 2.9899, Training Accuracy: 99.12%\nEpoch: 10, Training Loss: 1.3889, Training Accuracy: 99.53%\nTesting Model!!!\nTesting Loss: 2.0182, Testing Accuracy: 98.48%\nFor dataset 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 28.2759, Training Accuracy: 88.77%\nEpoch: 2, Training Loss: 15.9748, Training Accuracy: 94.05%\nEpoch: 3, Training Loss: 11.8024, Training Accuracy: 95.36%\nEpoch: 4, Training Loss: 7.2524, Training Accuracy: 97.55%\nEpoch: 5, Training Loss: 5.8023, Training Accuracy: 98.07%\nEpoch: 6, Training Loss: 4.3281, Training Accuracy: 98.54%\nEpoch: 7, Training Loss: 4.5990, Training Accuracy: 98.37%\nEpoch: 8, Training Loss: 3.2492, Training Accuracy: 98.92%\nEpoch: 9, Training Loss: 2.4625, Training Accuracy: 99.15%\nEpoch: 10, Training Loss: 1.8935, Training Accuracy: 99.24%\nTesting Model!!!\nTesting Loss: 2.8811, Testing Accuracy: 96.85%\nFor dataset 5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 60.0413, Training Accuracy: 71.85%\nEpoch: 2, Training Loss: 52.4845, Training Accuracy: 71.32%\nEpoch: 3, Training Loss: 49.8085, Training Accuracy: 73.02%\nEpoch: 4, Training Loss: 48.2105, Training Accuracy: 73.05%\nEpoch: 5, Training Loss: 46.2056, Training Accuracy: 72.40%\nEpoch: 6, Training Loss: 45.5479, Training Accuracy: 73.60%\nEpoch: 7, Training Loss: 43.6663, Training Accuracy: 73.37%\nEpoch: 8, Training Loss: 43.5776, Training Accuracy: 73.45%\nEpoch: 9, Training Loss: 42.6490, Training Accuracy: 74.24%\nEpoch: 10, Training Loss: 43.6319, Training Accuracy: 73.42%\nTesting Model!!!\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Testing Loss: 11.4554, Testing Accuracy: 73.51%\nFor dataset 6\nNo. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 23.9871, Training Accuracy: 92.68%\nEpoch: 2, Training Loss: 17.4115, Training Accuracy: 93.64%\nEpoch: 3, Training Loss: 12.5262, Training Accuracy: 96.00%\nEpoch: 4, Training Loss: 8.7739, Training Accuracy: 97.08%\nEpoch: 5, Training Loss: 8.5135, Training Accuracy: 97.55%\nEpoch: 6, Training Loss: 6.5651, Training Accuracy: 97.75%\nEpoch: 7, Training Loss: 4.8002, Training Accuracy: 98.45%\nEpoch: 8, Training Loss: 4.2814, Training Accuracy: 98.69%\nEpoch: 9, Training Loss: 3.4242, Training Accuracy: 98.86%\nEpoch: 10, Training Loss: 3.1135, Training Accuracy: 98.86%\nTesting Model!!!\nTesting Loss: 5.7325, Testing Accuracy: 95.68%\nFor dataset 7\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"No. of GPUs:  2\nTraining Model!!!\nEpoch: 1, Training Loss: 55.0930, Training Accuracy: 71.88%\nEpoch: 2, Training Loss: 47.7746, Training Accuracy: 72.26%\nEpoch: 3, Training Loss: 45.0720, Training Accuracy: 73.40%\nEpoch: 4, Training Loss: 42.9029, Training Accuracy: 73.54%\nEpoch: 5, Training Loss: 43.1235, Training Accuracy: 72.75%\nEpoch: 6, Training Loss: 42.3170, Training Accuracy: 73.19%\nEpoch: 7, Training Loss: 41.7533, Training Accuracy: 73.86%\nEpoch: 8, Training Loss: 41.5028, Training Accuracy: 74.74%\nEpoch: 9, Training Loss: 41.2904, Training Accuracy: 73.16%\nEpoch: 10, Training Loss: 40.5745, Training Accuracy: 74.36%\nTesting Model!!!\nTesting Loss: 13.8808, Testing Accuracy: 73.86%\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"for i, model in enumerate(models):\n    torch.save(model.state_dict(), f'model_{i}.pth')","metadata":{"id":"JcOV2eUuOUMC","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T06:45:31.543688Z","iopub.execute_input":"2025-01-30T06:45:31.544015Z","iopub.status.idle":"2025-01-30T06:45:36.772660Z","shell.execute_reply.started":"2025-01-30T06:45:31.543991Z","shell.execute_reply":"2025-01-30T06:45:36.771728Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Load the model\nnew_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')\n\n# Load the saved state dict\nstate_dict = torch.load('/kaggle/working/model_0.pth')\n\n# Remove 'module.' prefix from the state dict keys\nnew_state_dict = {}\nfor k, v in state_dict.items():\n    new_key = k[7:] if k.startswith('module.') else k  # Remove 'module.' prefix if it exists\n    new_state_dict[new_key] = v\n\n# Load the corrected state dict\nnew_model.load_state_dict(new_state_dict)\n\nnew_model = torch.nn.DataParallel(new_model)\nnew_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T07:03:12.436222Z","iopub.execute_input":"2025-01-30T07:03:12.436578Z","iopub.status.idle":"2025-01-30T07:03:13.527235Z","shell.execute_reply.started":"2025-01-30T07:03:12.436553Z","shell.execute_reply":"2025-01-30T07:03:13.526238Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-39-85e226b7fa48>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load('/kaggle/working/model_0.pth')\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): BertForSequenceClassification(\n    (bert): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSdpaSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n    (classifier): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"def test(model, test_loader):\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        correct_predictions = 0\n        total_predictions = 0\n        running_loss = 0.0\n        count = 0\n\n        for input_ids, attention_mask, labels in test_loader:\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            output = model(input_ids, attention_mask)\n            logits = output.logits\n            loss = criterion(logits, labels)\n\n            running_loss += loss.item()\n            count += 1\n\n            _, predicted = torch.max(logits, 1)\n            total_predictions += labels.size(0)\n            correct_predictions += (predicted == labels).sum().item()\n\n        accuracy = correct_predictions / total_predictions * 100\n        print(f'Testing Loss: {running_loss:.4f}, Testing Accuracy: {accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T07:04:40.108212Z","iopub.execute_input":"2025-01-30T07:04:40.108518Z","iopub.status.idle":"2025-01-30T07:04:40.114337Z","shell.execute_reply.started":"2025-01-30T07:04:40.108495Z","shell.execute_reply":"2025-01-30T07:04:40.113390Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"x = df_0['code']\ny = df_0[f'label_encoded_0']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=True, random_state=42, stratify=y)\n\ntest_loader = get_dataloader(x_test, y_test)\ntest(new_model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T07:04:40.405700Z","iopub.execute_input":"2025-01-30T07:04:40.405998Z","iopub.status.idle":"2025-01-30T07:05:07.277724Z","shell.execute_reply.started":"2025-01-30T07:04:40.405974Z","shell.execute_reply":"2025-01-30T07:05:07.276893Z"}},"outputs":[{"name":"stdout","text":"Testing Loss: 3.6266, Testing Accuracy: 96.50%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}